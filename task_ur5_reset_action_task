import argparse
import os
import torch

import numpy as np
from envs.visual_ur5_min_time_reacher.env import VisualReacherMinTimeEnv
from datetime import datetime
from logger import Logger
from algo.comm import MODE
from algo.onboard_wrapper import OnboardWrapper
from algo.sac_rad_agent import SACRADLearner, SACRADPerformer
from envs.visual_ur5_reacher.configs.ur5_config import config
from envs.visual_ur5_min_time_reacher.env import VisualReacherMinTimeEnv
from remote_learner_ur5 import MonitorTarget
import numpy as np
import cv2, utils, time

# from rl_suite.algo.sac_rad import ResetSACRADAgent
# from rl_suite.algo.replay_buffer import SACReplayBuffer, SACRADBuffer
# from rl_suite.experiment import Experiment


class SACExperiment():
    def __init__(self):
        self.run_id = datetime.now().strftime("%Y%m%d-%H%M%S")
        self.args = self.parse_args()
        self.env = self.make_env()
        base_fname = os.path.join(os.path.dirname(os.path.abspath(__file__)), self.args.work_dir, "{}_sac_{}_{}-{}".format(
            self.run_id, self.env_name, self.args.description, self.args.seed))
        self.fname = base_fname + ".txt"
        self.plt_fname = base_fname + ".png"
        self.base_fname = base_fname

        print('-'*50)
        print("{}-{}".format(self.run_id, base_fname))
        print('-'*50)               

    def make_env(self):
        env = VisualReacherMinTimeEnv(
            setup = self.args.setup,
            ip = self.args.ur5_ip,
            seed = self.args.seed,
            camera_id = self.args.camera_id,
            image_width = self.args.image_width,
            image_height = self.args.image_height,
            target_type = self.args.target_type,
            image_history = self.args.image_history,
            joint_history = self.args.joint_history,
            episode_length = self.args.episode_length_time,
            dt = self.args.dt,
            size_tol = self.args.size_tol,
            center_tol = self.args.center_tol,
            reward_tol = self.args.reward_tol,
            )
        self.env_name = self.args.env
        return env

    def parse_args(self):
        parser = argparse.ArgumentParser()
        # Task
        dd parser.add_argument('--setup', default='Visual-UR5-min-time')
        dd parser.add_argument('--env', default="Visual-UR5-min-time", type=str, help="e.g., 'ball_in_cup', 'sparse_reacher', 'Hopper-v2' ")
        dd parser.add_argument('--ur5_ip', default='129.128.159.210', type=str)
        dd parser.add_argument('--camera_id', default=0, type=int)
        dd parser.add_argument('--image_width', default=160, type=int)
        dd parser.add_argument('--image_height', default=90, type=int)
        dd parser.add_argument('--target_type', default='reward', type=str)
        dd parser.add_argument('--image_history', default=3, type=int)
        dd parser.add_argument('--dt', default=0.04, type=float)
        dd parser.add_argument('--size_tol', default=0.015, type=float)
        dd parser.add_argument('--center_tol', default=0.1, type=float)
        dd parser.add_argument('--reward_tol', default=2.0, type=float)
        dd parser.add_argument('--seed', default=0, type=int, help="Seed for random number generator")       
        dd parser.add_argument('--N', default=120000, type=int, help="# timesteps for the run")
        dd parser.add_argument('--timeout', default=500, type=int, help="Timeout for the env")
        # Reset threshold
        dd parser.add_argument('--reset_thresh', default=0.9, type=float, help="Action threshold between [-1, 1]")
        # Algorithm
        dd parser.add_argument('--replay_buffer_capacity', default=100000, type=int)
        dd parser.add_argument('--init_steps', default=5000, type=int)
        dd parser.add_argument('--update_every', default=50, type=int)
        dd parser.add_argument('--update_epochs', default=50, type=int)
        dd parser.add_argument('--batch_size', default=256, type=int)
        dd parser.add_argument('--gamma', default=0.995, type=float, help="Discount factor")
        dd parser.add_argument('--bootstrap_terminal', default=0, type=int, help="Bootstrap on terminal state")
        dd parser.add_argument('--async_mode', default=True, action='store_true')
        dd parser.add_argument('--max_updates_per_step', default=0.6, type=float)
        ## Actor
        dd parser.add_argument('--actor_lr', default=3e-4, type=float)
        dd parser.add_argument('--actor_update_freq', default=1, type=int)
        ## Critic
        dd parser.add_argument('--critic_lr', default=3e-4, type=float)
        dd parser.add_argument('--critic_tau', default=0.01, type=float)
        dd parser.add_argument('--critic_target_update_freq', default=1, type=int)
        ## Entropy
        dd parser.add_argument('--init_temperature', default=0.1, type=float)
        dd parser.add_argument('--alpha_lr', default=1e-4, type=float)
        ## Encoder
        dd parser.add_argument('--encoder_tau', default=0.05, type=float)
        parser.add_argument('--l2_reg', default=0, type=float, help="L2 regularization coefficient")
        # RAD
        dd parser.add_argument('--rad_offset', default=0.01, type=float)
        parser.add_argument('--freeze_cnn', default=0, type=int)        
        # Misc
        dd parser.add_argument('--work_dir', default='./results', type=str)
        dd parser.add_argument('--save_tb', default=False, action='store_true')
        dd parser.add_argument('--checkpoint', default=5000, type=int, help="Save plots and rets every checkpoint")
        dd parser.add_argument('--load_model', default=-1, type=int)
        dd parser.add_argument('--device', default="cuda", type=str)
        dd parser.add_argument('--description', default='size_margin=20_reset_action', type=str)
        # agent
        dd parser.add_argument('--remote_ip', default='localhost', type=str)
        dd parser.add_argument('--port', default=9876, type=int)
        dd parser.add_argument('--mode', default='o', type=str, help="Modes in ['r', 'o', 'ro', 'e'] ")
    
        args = parser.parse_args()

        # TODO: Fix this hardcoding by providing choice of network architectures
        args.net_params = {
            # Spatial softmax encoder net params
            'conv': [
                # in_channel, out_channel, kernel_size, stride
                [-1, 32, 3, 2],
                [32, 32, 3, 2],
                [32, 32, 3, 2],
                [32, 32, 3, 1],
            ],
        
            'latent': 50,

            'mlp': [
                [-1, 1024],
                [1024, 1024],
                [1024, 1024], # more layer?
                [1024, -1]
            ],
        }            

        if args.device == 'cpu':
            args.device = torch.device("cpu")
        else:
            args.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        return args

    def run(self):
        args = self.args
        if args.mode == 'r':
            mode = MODE.REMOTE_ONLY
        elif args.mode == 'o':
            mode = MODE.LOCAL_ONLY
            mt = MonitorTarget()
            
        elif args.mode == 'ro':
            mode = MODE.ONBOARD_REMOTE
        elif args.mode == 'e':
            mt = MonitorTarget()
            mt.reset_plot()
            mode = MODE.EVALUATION
        else:
            raise  NotImplementedError()

        if args.device == '':
            args.device = 'cuda' if torch.cuda.is_available() else 'cpu'

        args.work_dir += f'/results/{args.env_name}_' \
                        f'dt={args.dt}_bs={args.batch_size}_' \
                        f'target_type={args.target_type}_'\
                        f'dim={args.image_width}*{args.image_height}_{args.seed}_'+args.appendix

        args.model_dir = args.work_dir+'/model'

        if mode == MODE.LOCAL_ONLY:
            utils.make_dir(args.work_dir)
            utils.make_dir(args.model_dir)
            L = Logger(args.work_dir, use_tb=args.save_tb)

        if mode == MODE.EVALUATION:
            args.image_dir = args.work_dir+'image'
            utils.make_dir(args.image_dir)

        env = VisualReacherMinTimeEnv(
            setup = args.setup,
            ip = args.ur5_ip,
            seed = args.seed,
            camera_id = args.camera_id,
            image_width = args.image_width,
            image_height = args.image_height,
            target_type = args.target_type,
            image_history = args.image_history,
            joint_history = args.joint_history,
            episode_length = args.episode_length_time,
            dt = args.dt,
            size_tol = args.size_tol,
            center_tol = args.center_tol,
            reward_tol = args.reward_tol,
        )
        # Reproducibility
        self.set_seed()

        mt.reset_plot()
        mt.reset_plot()
        mt.reset_plot()
        mt.reset_plot()

        input('go?')
        image, prop = env.reset()
        image_to_show = np.transpose(image, [1, 2, 0])
        image_to_show = image_to_show[:,:,-3:]
        cv2.imshow('raw', image_to_show)
        cv2.waitKey(0)

        args.image_shape = env.image_space.shape
        args.proprioception_shape = env.proprioception_space.shape
        args.action_shape = env.action_space.shape
        args.env_action_space = env.action_space
        args.net_params = config

        episode_length_step = int(args.timeout / args.dt)
        agent = OnboardWrapper(episode_length_step, mode, remote_ip=args.remote_ip, port=args.port)
        agent.send_data(args)
        agent.init_performer(SACRADPerformer, args)
        agent.init_learner(SACRADLearner, args, agent.performer)

        # sync initial weights with remote
        agent.apply_remote_policy(block=True)

        if args.load_model > -1:
            agent.load_policy_from_file(args.model_dir, args.load_model)

        if mode == MODE.EVALUATION:
            episode_image_dir = utils.make_dir(os.path.join(args.image_dir, str(episode)))
        # First inference took a while (~1 min), do it before the agent-env interaction loop
        if mode != MODE.REMOTE_ONLY:
            agent.performer.sample_action((image, prop), args.init_steps+1)

        if mode == MODE.EVALUATION and args.load_model > -1:
            args.init_steps = 0

        # Experiment block starts
        ret = 0
        step = 0 # why is it needed
        rets = []
        ep_lens = []
        n_resets = []
        i_episode = 0
        n_reset = 0

        agent.send_init_ob((image, prop))
        success = 0
        start_time = time.time()
        for t in range(self.args.N):
            image_to_show = np.transpose(image, [1, 2, 0])
            image_to_show = image_to_show[:,:,-3:]
            cv2.imshow('raw', image_to_show)
            cv2.waitKey(1)

            # Select an action
            action = agent.sample_action((image, prop), step) 
            x_action = action[:self.args.action_dim]
            reset_action = action[-1]

            # Reset action
            if reset_action > self.args.reset_thresh: 
                n_reset += 1
                t += 10 # why?
                step += 10 # why +10?          
                next_image, next_prop = self.env.reset()
                r = -1 # needs to change
                done = False
                infos = "Agent chose to reset itself"
            else:
                next_image, next_prop, r, done, infos = self.env.step(x_action)
                
            agent.push_sample((image, prop), action, r, (next_image, next_prop), done)                
            # if t % 100 == 0:
                # print("Step: {}, Obs: {}, Action: {}, Reward: {:.2f}, Done: {}".format(
                    # t, obs, action, r, done))
            image = next_image
            prop = next_prop
            ####### End

            # Log
            ret += r
            step += 1
            if done:    # Bootstrap on timeout
                i_episode += 1
                rets.append(ret)
                ep_lens.append(step)
                n_resets.append(n_reset)
                print("Episode {} ended after {} steps with return {:.2f}. # resets: {}. Total steps: {}".format(
                    i_episode, step, ret, n_reset, t))
                
                ret = 0
                step = 0
                n_reset = 0
                image, prop = self.env.reset()

            stat = agent.update_policy(step)
            if stat is not None:
                for k, v in stat.items():
                    L.log(k, v, step)

def main():
    runner = SACExperiment()
    runner.run()


if __name__ == "__main__":
    main()